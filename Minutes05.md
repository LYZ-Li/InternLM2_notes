
# LMDeploy 量化部署 LLM 实践笔记

## 1. 大模型部署背景

### 定义

**模型部署**
- 在软件工程中，部署通常指的是将开发完毕的软件投入使用的过程。
- 在人工智能领域，模型部署是实现深度学习算法落地应用的关键步骤。简单来说，模型部署就是将训练好的深度学习模型在特定环境中运行的过程。

> 模型部署不仅是技术实现的终点，更是应用价值的起点。它将理论和实验成果转化为实际应用，推动科技进步和社会发展。

### 场景
- 服务器端：CPU部署，单GPU/TPU/NPU部署，多卡/集群部署。
- 移动端/边缘端：移动机器人，手机等。

> 不同场景的部署需求各异，未来的发展方向可能是更灵活和高效的异构计算架构，以适应多样化的应用需求。

## 2. 大模型部署方法

### 模型剪枝 (Pruning)

**非结构化剪枝**：
- SparseGPT
- LoRAPrune
- Wanda

**结构化剪枝**：
- LLM-Pruner

**经典文献**：
1. Frantar, E., & Alistarh, D. "Sparsegpt: Massive language models can be accurately pruned in one-shot", https://proceedings.mlr.press/v202/frantar23a.html 

> 剪枝技术在保证模型性能的同时大幅度减少计算资源的消耗，是实现大规模模型高效部署的重要手段。未来，如何智能化地自动选择剪枝策略将是一个重要研究方向。

### 知识蒸馏 (Knowledge Distillation, KD)

**方法**：
- 上下文学习 (ICL)：ICL distillation
- 思维链 (CoT)：MT-COT，Fine-tune-CoT
- 指令跟随 (IF)：LaMini-LM


> 知识蒸馏通过教师模型和学生模型的互动优化了模型性能。未来可能发展出更加智能和高效的蒸馏策略，使轻量化模型达到更高的精度和效率。

### 量化 (Quantization)

**方法**：
- 量化感知训练 (QAT)：LLM-QAT
- 量化感知微调 (QAF)：PEQA，QLORA
- 训练后量化 (PTQ)：LLM.int8，AWQ

> 量化技术显著降低了模型的计算和存储成本，是部署大模型的关键技术。未来，量化方法将更加智能和多样化，以适应不同应用场景的需求。

## 3. LMDeploy简介

**核心功能**：
- 高效推理：LMDeploy开发了Continuous Batch，Blocked K/V Cache，动态拆分和融合，张量并行，高效的计算kernel等重要特性。
- 可靠量化：支持权重量化和k/v量化。
- 便捷服务：支持多模型在多机、多卡上的推理服务。
- 有状态推理：通过缓存多轮对话过程中的Attention的k/v，记住对话历史，从而避免重复处理历史会话。


> LMDeploy作为大模型部署的一站式解决方案，体现了高效、可靠和便捷的优势。未来，随着技术的发展，可能会加入更多智能化管理和优化功能，提高部署的自动化程度和灵活性。

## 4. 动手实践环节


