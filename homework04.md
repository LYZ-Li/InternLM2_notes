[åŸºç¡€ä½œä¸š]( ##åŸºç¡€ä½œä¸š)
[è¿›é˜¶ä½œä¸š]( ## è¿›é˜¶ä½œä¸š)
- [éƒ¨ç½²åˆ°OpenCLab](###éƒ¨ç½²åˆ°OpenCLab)
- [å¤ç°å¤šæ¨¡æ€å¾®è°ƒ](###å¤ç°å¤šæ¨¡æ€å¾®è°ƒ)
# ç¬¬ 4 èŠ‚è¯¾ä½œä¸š

> è®°å½•å¤ç°è¿‡ç¨‹å¹¶æˆªå›¾
> 
> ## åŸºç¡€ä½œä¸šï¼ˆç»“è¥å¿…åšï¼‰
> 
> - è®­ç»ƒè‡ªå·±çš„å°åŠ©æ‰‹è®¤çŸ¥ï¼ˆè®°å½•å¤ç°è¿‡ç¨‹å¹¶æˆªå›¾ï¼‰
> 
> ## è¿›é˜¶ä½œä¸š
> 
> - å°†è‡ªæˆ‘è®¤çŸ¥çš„æ¨¡å‹ä¸Šä¼ åˆ° OpenXLabï¼Œå¹¶å°†åº”ç”¨éƒ¨ç½²åˆ° OpenXLabï¼ˆä¼˜ç§€å­¦å‘˜å¿…åšï¼‰
> - å¤ç°å¤šæ¨¡æ€å¾®è°ƒï¼ˆä¼˜ç§€å­¦å‘˜å¿…åšï¼‰
> 
> 
> OpenXLab éƒ¨ç½²æ•™ç¨‹ï¼šhttps://github.com/InternLM/Tutorial/tree/camp2/tools/openxlab-deploy

# åŸºç¡€ä½œä¸š
ç”¨ `QLoRA` çš„æ–¹å¼æ¥å¾®è°ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹ï¼

## 1 å¼€å‘æœºå‡†å¤‡
å‰å¾€ [InternStudio](https://studio.intern-ai.org.cn/) ä¸­åˆ›å»ºä¸€ä¸ªå¼€å‘æœºè¿›è¡Œä½¿ç”¨ã€‚
## 2 å¿«é€Ÿä¸Šæ‰‹
ç®€å•äº†è§£ä¸€ä¸‹ XTuner çš„è¿è¡ŒåŸç†ã€‚

<img width="3216" alt="XTunerFlow1" src="images/image_04_06.png">

1. **ç¯å¢ƒå®‰è£…**ï¼šå‡å¦‚æˆ‘ä»¬æƒ³è¦ç”¨ XTuner è¿™æ¬¾ç®€å•æ˜“ä¸Šæ‰‹çš„å¾®è°ƒå·¥å…·åŒ…æ¥å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¯ï¼Œé‚£æˆ‘ä»¬æœ€æœ€æœ€å…ˆå¼€å§‹çš„ç¬¬ä¸€æ­¥å¿…ç„¶å°±æ˜¯å®‰è£…XTunerï¼å®‰è£…åŸºç¡€çš„å·¥å…·æ˜¯ä¸€åˆ‡çš„å‰æï¼Œåªæœ‰å®‰è£…äº† XTuner åœ¨æˆ‘ä»¬æœ¬åœ°åæˆ‘ä»¬æ‰èƒ½å¤Ÿå»æ€è€ƒè¯´å…·ä½“æ€ä¹ˆæ“ä½œã€‚

2. **å‰æœŸå‡†å¤‡**ï¼šé‚£åœ¨å®Œæˆäº†å®‰è£…åï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥å°±éœ€è¦å»æ˜ç¡®æˆ‘ä»¬è‡ªå·±çš„å¾®è°ƒç›®æ ‡äº†ã€‚æˆ‘ä»¬æƒ³è¦åˆ©ç”¨å¾®è°ƒåšä¸€äº›ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Œé‚£æˆ‘ä¸ºäº†åšåˆ°è¿™ä¸ªäº‹æƒ…æˆ‘æœ‰å“ªäº›ç¡¬ä»¶çš„èµ„æºå’Œæ•°æ®å‘¢ï¼Ÿå‡å¦‚æˆ‘ä»¬æœ‰å¯¹äºä¸€ä»¶äº‹æƒ…ç›¸å…³çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜æœ‰è¶³å¤Ÿçš„ç®—åŠ›èµ„æºï¼Œé‚£å½“ç„¶å¾®è°ƒå°±æ˜¯ä¸€ä»¶æ°´åˆ°æ¸ æˆçš„äº‹æƒ…ã€‚å°±åƒ OpenAI ä¸å°±æ˜¯å¦‚æ­¤å—ï¼Ÿä½†æ˜¯å¯¹äºæ™®é€šçš„å¼€å‘è€…è€Œè¨€ï¼Œåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å°±éœ€è¦è€ƒè™‘æ€ä¹ˆé‡‡é›†æ•°æ®ï¼Œç”¨ä»€ä¹ˆæ ·çš„æ‰‹æ®µå’Œæ–¹å¼æ¥è®©æ¨¡å‹æœ‰æ›´å¥½çš„æ•ˆæœã€‚

3. **å¯åŠ¨å¾®è°ƒ**ï¼šåœ¨ç¡®å®šäº†è‡ªå·±çš„å¾®è°ƒç›®æ ‡åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ XTuner çš„é…ç½®åº“ä¸­æ‰¾åˆ°åˆé€‚çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œå¯¹åº”çš„ä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆåå³å¯ä¸€é”®å¯åŠ¨è®­ç»ƒï¼è®­ç»ƒå¥½çš„æ¨¡å‹ä¹Ÿå¯ä»¥ä»…ä»…é€šè¿‡åœ¨ç»ˆç«¯è¾“å…¥ä¸€è¡ŒæŒ‡ä»¤æ¥å®Œæˆè½¬æ¢å’Œéƒ¨ç½²å·¥ä½œï¼


### 2.1 ç¯å¢ƒå®‰è£…
é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆå®‰è£…ä¸€ä¸ª XTuner çš„æºç åˆ°æœ¬åœ°æ¥æ–¹ä¾¿åç»­çš„ä½¿ç”¨ã€‚
```bash
# å¦‚æœä½ æ˜¯åœ¨ InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch çš„ç¯å¢ƒï¼š
# pytorch    2.0.1   py3.10_cuda11.7_cudnn8.5.0_0

studio-conda xtuner0.1.17
# å¦‚æœä½ æ˜¯åœ¨å…¶ä»–å¹³å°ï¼š
# conda create --name xtuner0.1.17 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate xtuner0.1.17
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
mkdir -p /root/xtuner0117 && cd /root/xtuner0117

# æ‹‰å– 0.1.17 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.17  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.15 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd /root/xtuner0117/xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```
> å‡å¦‚é€Ÿåº¦å¤ªæ…¢å¯ä»¥ `Ctrl + C` é€€å‡ºåæ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`

æ¥ä¸‹æ¥å°±å¯ä»¥è¿›å…¥æˆ‘ä»¬çš„ç¬¬äºŒæ­¥ï¼Œå‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„æ•°æ®é›†ã€æ¨¡å‹å’Œé…ç½®æ–‡ä»¶ï¼

![](images/image_04_09.png)
### 2.2 å‰æœŸå‡†å¤‡

#### 2.2.1 æ•°æ®é›†å‡†å¤‡

é¦–å…ˆæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥å­˜æ”¾å¾®è°ƒæ•°æ®ã€‚

```bash
# å‰åŠéƒ¨åˆ†æ˜¯åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒååŠéƒ¨åˆ†æ˜¯è¿›å…¥è¯¥æ–‡ä»¶å¤¹ã€‚
mkdir -p /root/ft && cd /root/ft

# åœ¨ftè¿™ä¸ªæ–‡ä»¶å¤¹é‡Œå†åˆ›å»ºä¸€ä¸ªå­˜æ”¾æ•°æ®çš„dataæ–‡ä»¶å¤¹
mkdir -p /root/ft/data && cd /root/ft/data
```

ä¹‹åæˆ‘ä»¬å¯ä»¥åœ¨ `data` ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ª `generate_data.py` æ–‡ä»¶ï¼Œå°†ä»¥ä¸‹ä»£ç å¤åˆ¶è¿›å»ï¼Œç„¶åè¿è¡Œè¯¥è„šæœ¬å³å¯ç”Ÿæˆæ•°æ®é›†ã€‚å‡å¦‚æƒ³è¦åŠ å¤§å‰‚é‡è®©ä»–èƒ½å¤Ÿå®Œå®Œå…¨å…¨è®¤è¯†åˆ°ä½ çš„èº«ä»½ï¼Œé‚£æˆ‘ä»¬å¯ä»¥å§ `n` çš„å€¼è°ƒå¤§ä¸€ç‚¹ã€‚

```bash
# åˆ›å»º `generate_data.py` æ–‡ä»¶
touch /root/ft/data/generate_data.py
```

æ‰“å¼€è¯¥ python æ–‡ä»¶åå°†[generate_data.py](codes_04/generate_data.py)ä¸‹é¢çš„å†…å®¹å¤åˆ¶è¿›å»ã€‚



å¹¶å°†æ–‡ä»¶ `name` åé¢çš„å†…å®¹ä¿®æ”¹ä¸ºä½ çš„åç§°ã€‚æ¯”å¦‚è¯´æˆ‘æ˜¯å°èœé¸¡çš„è¯å°±æ˜¯ï¼š

```diff
# å°†å¯¹åº”çš„nameè¿›è¡Œä¿®æ”¹ï¼ˆåœ¨ç¬¬4è¡Œçš„ä½ç½®ï¼‰
- name = 'ä¸è¦å§œè‘±è’œå¤§ä½¬'
+ name = "å°èœé¸¡"
```

ä¿®æ”¹å®Œæˆåè¿è¡Œ `generate_data.py` æ–‡ä»¶å³å¯ã€‚

``` bash
# ç¡®ä¿å…ˆè¿›å…¥è¯¥æ–‡ä»¶å¤¹
cd /root/ft/data

# è¿è¡Œä»£ç 
python /root/ft/data/generate_data.py
```
å¯ä»¥çœ‹åˆ°åœ¨dataçš„è·¯å¾„ä¸‹ä¾¿ç”Ÿæˆäº†ä¸€ä¸ªåä¸º `personal_assistant.json` çš„æ–‡ä»¶ï¼Œè¿™æ ·æˆ‘ä»¬æœ€å¯ç”¨äºå¾®è°ƒçš„æ•°æ®é›†å°±å‡†å¤‡å¥½å•¦ï¼é‡Œé¢å°±åŒ…å«äº† 5000 æ¡ `input` å’Œ `output` çš„æ•°æ®å¯¹ã€‚å‡å¦‚ æˆ‘ä»¬è®¤ä¸º 5000 æ¡ä¸å¤Ÿçš„è¯ä¹Ÿå¯ä»¥è°ƒæ•´æ–‡ä»¶ä¸­ç¬¬6è¡Œ `n` çš„å€¼å“¦ï¼

```
|-- data/
    |-- personal_assistant.json
    |-- generate_data.py
```

<details>
<summary>æ–‡ä»¶ç»“æ„æ ‘ä»£ç </summary>

[tree.py](/tree.py)  
æ–‡ä»¶ç»“æ„æ ‘ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œä½¿ç”¨æ–¹æ³•ä¸ºåœ¨ç»ˆç«¯è°ƒç”¨è¯¥ä»£ç çš„åŒæ—¶åœ¨åæ–¹è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ã€‚

æ¯”å¦‚è¯´æˆ‘è¦æ‰“å° `data` çš„æ–‡ä»¶ç»“æ„æ ‘ï¼Œå‡è®¾æˆ‘çš„ä»£ç æ–‡ä»¶ä¿å­˜åœ¨ `/root/tree.py` ï¼Œé‚£æˆ‘å°±è¦åœ¨ç»ˆç«¯è¾“å…¥ `python /root/tree.py /root/ft/data` 


</details>

> é™¤äº†æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬çš„æ•°æ®é›†ï¼Œå…¶å®ç½‘ä¸Šä¹Ÿæœ‰å¤§é‡çš„å¼€æºæ•°æ®é›†å¯ä»¥ä¾›æˆ‘ä»¬è¿›è¡Œä½¿ç”¨ã€‚æœ‰äº›æ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨å¼€æºæ•°æ®é›†çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€äº›æˆ‘ä»¬è‡ªå·±ç‹¬æœ‰çš„æ•°æ®é›†ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚

#### 2.2.2 æ¨¡å‹å‡†å¤‡

ä½¿ç”¨ InternLM æœ€æ–°æ¨å‡ºçš„å°æ¨¡å‹ `InterLM2-Chat-1.8B` æ¥å®Œæˆæ­¤æ¬¡çš„å¾®è°ƒæ¼”ç¤ºã€‚

åœ¨ InternStudio ä¸Šå¯ä»¥ä¸ç”¨é€šè¿‡ OpenXLab æˆ–è€… Modelscope è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ã€‚

ä¸ºé¿å…å­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·æ—¢èŠ‚çœäº†ç©ºé—´ï¼Œä¹Ÿä¾¿äºç®¡ç†ã€‚

```bash
# åˆ é™¤/root/ft/modelç›®å½•
rm -rf /root/ft/model

# åˆ›å»ºç¬¦å·é“¾æ¥
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/ft/model
```
æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`/root/ft/model` å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘ `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` çš„ä½ç½®ã€‚

è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬è®¿é—® `/root/ft/model` æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è®¿é—® `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` ç›®å½•ä¸‹çš„å†…å®¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ— éœ€å¤åˆ¶ä»»ä½•æ•°æ®ï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œåç»­çš„å¾®è°ƒæ“ä½œï¼Œä»è€ŒèŠ‚çœå­˜å‚¨ç©ºé—´å¹¶ç®€åŒ–æ–‡ä»¶ç®¡ç†ã€‚

åœ¨è¯¥æƒ…å†µä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚
```
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- .mdl
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- .msc
    |-- special_tokens_map.json
    |-- .mv
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```
![](images/image_04_09.png)

#### 2.2.3 é…ç½®æ–‡ä»¶é€‰æ‹©

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š
> å¼€ç®±å³ç”¨æ„å‘³ç€å‡å¦‚èƒ½å¤Ÿè¿æ¥ä¸Š Huggingface ä»¥åŠæœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œå…¶å®å°±å¯ä»¥ç›´æ¥è¿è¡Œè¿™äº›é…ç½®æ–‡ä»¶ï¼ŒXTunerå°±èƒ½å¤Ÿç›´æ¥ä¸‹è½½å¥½è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ç„¶åå¼€å§‹è¿›è¡Œå¾®è°ƒ
```Bash
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®æ–‡ä»¶
# xtuner list-cfg

# å‡å¦‚æˆ‘ä»¬æƒ³æ‰¾åˆ° internlm2-1.8b æ¨¡å‹é‡Œæ”¯æŒçš„é…ç½®æ–‡ä»¶
xtuner list-cfg -p internlm2_1_8b
```
è™½ç„¶æˆ‘ä»¬ç”¨çš„æ•°æ®é›†å¹¶ä¸æ˜¯ `alpaca` è€Œæ˜¯æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬åˆ¶ä½œçš„å°åŠ©æ‰‹æ•°æ®é›† ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡ `QLoRA` çš„æ–¹å¼å¯¹ `internlm2-chat-1.8b` è¿›è¡Œå¾®è°ƒã€‚è€Œæœ€ç›¸è¿‘çš„é…ç½®æ–‡ä»¶åº”è¯¥å°±æ˜¯ `internlm2_1_8b_qlora_alpaca_e3` ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ‹·è´è¿™ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š
```Bash
# åˆ›å»ºä¸€ä¸ªå­˜æ”¾ config æ–‡ä»¶çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/config

# ä½¿ç”¨ XTuner ä¸­çš„ copy-cfg åŠŸèƒ½å°† config æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„ä½ç½®
xtuner copy-cfg internlm2_1_8b_qlora_alpaca_e3 /root/ft/config
```
> è¿™é‡Œæˆ‘ä»¬å°±ç”¨åˆ°äº† XTuner å·¥å…·ç®±ä¸­çš„ç¬¬äºŒä¸ªå·¥å…· `copy-cfg` ï¼Œè¯¥å·¥å…·æœ‰ä¸¤ä¸ªå¿…é¡»è¦å¡«å†™çš„å‚æ•° `{CONFIG_NAME}` å’Œ `{SAVE_PATH}` ï¼Œåœ¨æˆ‘ä»¬çš„è¾“å…¥çš„è¿™ä¸ªæŒ‡ä»¤ä¸­ï¼Œæˆ‘ä»¬çš„ `{CONFIG_NAME}` å¯¹åº”çš„æ˜¯ä¸Šé¢æœç´¢åˆ°çš„ `internlm2_1_8b_qlora_alpaca_e3` ,è€Œ `{SAVE_PATH}` åˆ™å¯¹åº”çš„æ˜¯åˆšåˆšæ–°å»ºçš„ `/root/ft/config`ã€‚æˆ‘ä»¬å‡å¦‚éœ€è¦å¤åˆ¶å…¶ä»–çš„é…ç½®æ–‡ä»¶åªéœ€è¦ä¿®æ”¹è¿™ä¸¤ä¸ªå‚æ•°å³å¯å®ç°ã€‚
è¾“å…¥åæˆ‘ä»¬å°±èƒ½å¤Ÿçœ‹åˆ°åœ¨æˆ‘ä»¬çš„ `/root/ft/config` æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€ä¸ªåä¸º `internlm2_1_8b_qlora_alpaca_e3_copy.py` çš„æ–‡ä»¶äº†ã€‚
```
|-- config/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
```
#### 2.2.4 å°ç»“

ç»è¿‡äº†ä»¥ä¸Šçš„æ­¥éª¤åï¼Œæˆ‘ä»¬çš„ `ft` æ–‡ä»¶å¤¹é‡Œåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
```
|-- ft/
    |-- config/
        |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- model/
        |-- tokenizer.model
        |-- config.json
        |-- tokenization_internlm2.py
        |-- model-00002-of-00002.safetensors
        |-- tokenizer_config.json
        |-- model-00001-of-00002.safetensors
        |-- model.safetensors.index.json
        |-- configuration.json
        |-- special_tokens_map.json
        |-- modeling_internlm2.py
        |-- README.md
        |-- configuration_internlm2.py
        |-- generation_config.json
        |-- tokenization_internlm2_fast.py
    |-- data/
        |-- personal_assistant.json
        |-- generate_data.py
```
![](images/image_04_09.png)

### 2.3 é…ç½®æ–‡ä»¶ä¿®æ”¹

ç›´æ¥å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ° `/root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py` æ–‡ä»¶ä¸­ï¼ˆå…ˆ `Ctrl + A` é€‰ä¸­æ‰€æœ‰æ–‡ä»¶å¹¶åˆ é™¤åå†å°†ä»£ç å¤åˆ¶è¿›å»ï¼‰ã€‚  
[internlm2_1_8b_qlora_alpaca_e3_copy.py](codes_04/internlm2_1_8b_qlora_alpaca_e3_copy.py)
<details>
<summary><b>å‚æ•°ä¿®æ”¹ç»†èŠ‚</b></summary>

é¦–å…ˆåœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ Huggingface ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚
    
```diff
# ä¿®æ”¹æ¨¡å‹åœ°å€ï¼ˆåœ¨ç¬¬27è¡Œçš„ä½ç½®ï¼‰
- pretrained_model_name_or_path = 'internlm/internlm2-1_8b'
+ pretrained_model_name_or_path = '/root/ft/model'

# ä¿®æ”¹æ•°æ®é›†åœ°å€ä¸ºæœ¬åœ°çš„jsonæ–‡ä»¶åœ°å€ï¼ˆåœ¨ç¬¬31è¡Œçš„ä½ç½®ï¼‰
- alpaca_en_path = 'tatsu-lab/alpaca'
+ alpaca_en_path = '/root/ft/data/personal_assistant.json'
```

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¯¹ä¸€äº›é‡è¦çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ï¼ˆlrï¼‰ã€è®­ç»ƒçš„è½®æ•°ï¼ˆmax_epochsï¼‰ç­‰ç­‰ã€‚ç”±äºæˆ‘ä»¬è¿™æ¬¡åªæ˜¯ä¸€ä¸ªç®€å•çš„è®©æ¨¡å‹çŸ¥é“è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼Œå› æ­¤æˆ‘ä»¬çš„è®­ç»ƒè½®æ•°ä»¥åŠå•æ¡æ•°æ®æœ€å¤§çš„ Token æ•°ï¼ˆmax_lengthï¼‰éƒ½å¯ä»¥ä¸ç”¨é‚£ä¹ˆå¤§ã€‚

```diff
# ä¿®æ”¹max_lengthæ¥é™ä½æ˜¾å­˜çš„æ¶ˆè€—ï¼ˆåœ¨ç¬¬33è¡Œçš„ä½ç½®ï¼‰
- max_length = 2048
+ max_length = 1024

# å‡å°‘è®­ç»ƒçš„è½®æ•°ï¼ˆåœ¨ç¬¬44è¡Œçš„ä½ç½®ï¼‰
- max_epochs = 3
+ max_epochs = 2

# å¢åŠ ä¿å­˜æƒé‡æ–‡ä»¶çš„æ€»æ•°ï¼ˆåœ¨ç¬¬54è¡Œçš„ä½ç½®ï¼‰
- save_total_limit = 2
+ save_total_limit = 3
```

å¦å¤–ï¼Œä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner ä¹Ÿæ˜¯è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æ¯”å¦‚è¯´æˆ‘ä»¬è¿™é‡Œæ˜¯å¸Œæœ›åœ¨é—®å‡º â€œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±â€ æˆ–è€…è¯´ â€œä½ æ˜¯è°â€ çš„æ—¶å€™ï¼Œæ¨¡å‹èƒ½å¤Ÿç»™ä½ çš„å›å¤æ˜¯ â€œæˆ‘æ˜¯XXXçš„å°åŠ©æ‰‹...â€ è¿™æ ·çš„å›å¤ã€‚å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æ ¹æ®è¿™ä¸ªéœ€æ±‚è¿›è¡Œæ›´æ”¹ã€‚


``` diff
# ä¿®æ”¹æ¯å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆåœ¨ç¬¬57è¡Œçš„ä½ç½®ï¼‰
- evaluation_freq = 500
+ evaluation_freq = 300

# ä¿®æ”¹å…·ä½“è¯„ä¼°çš„é—®é¢˜ï¼ˆåœ¨ç¬¬59åˆ°61è¡Œçš„ä½ç½®ï¼‰
# å¯ä»¥è‡ªç”±æ‹“å±•å…¶ä»–é—®é¢˜
- evaluation_inputs = ['è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai']
+ evaluation_inputs = ['è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ æ˜¯è°', 'ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—']
```
è¿™æ ·ä¿®æ”¹å®Œååœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å°±ä¼šæ˜¾ç¤ºåœ¨å½“å‰çš„æƒé‡æ–‡ä»¶ä¸‹æ¨¡å‹å¯¹è¿™å‡ ä¸ªé—®é¢˜çš„å›å¤äº†ã€‚

ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†ä¸å†æ˜¯åŸæœ¬çš„ aplaca æ•°æ®é›†ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿè¦è¿›å…¥ PART 3 çš„éƒ¨åˆ†å¯¹ç›¸å…³çš„å†…å®¹è¿›è¡Œä¿®æ”¹ã€‚åŒ…æ‹¬è¯´æˆ‘ä»¬æ•°æ®é›†è¾“å…¥çš„ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹è€Œæ˜¯ä¸€ä¸ªå•çº¯çš„ json æ–‡ä»¶ä»¥åŠæˆ‘ä»¬çš„æ•°æ®é›†æ ¼å¼è¦æ±‚æ”¹ä¸ºæˆ‘ä»¬æœ€é€šç”¨çš„ OpenAI æ•°æ®é›†æ ¼å¼ã€‚

``` diff
# æŠŠ OpenAI æ ¼å¼çš„ map_fn è½½å…¥è¿›æ¥ï¼ˆåœ¨ç¬¬15è¡Œçš„ä½ç½®ï¼‰
- from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import openai_map_fn, template_map_fn_factory

# å°†åŸæœ¬æ˜¯ alpaca çš„åœ°å€æ”¹ä¸ºæ˜¯ json æ–‡ä»¶çš„åœ°å€ï¼ˆåœ¨ç¬¬102è¡Œçš„ä½ç½®ï¼‰
- dataset=dict(type=load_dataset, path=alpaca_en_path),
+ dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),

# å°† dataset_map_fn æ”¹ä¸ºé€šç”¨çš„ OpenAI æ•°æ®é›†æ ¼å¼ï¼ˆåœ¨ç¬¬105è¡Œçš„ä½ç½®ï¼‰
- dataset_map_fn=alpaca_map_fn,
+ dataset_map_fn=openai_map_fn,
```




</details>


è¿™ä¸€èŠ‚æˆ‘ä»¬è®²è¿°äº†å¾®è°ƒè¿‡ç¨‹ä¸­ä¸€äº›å¸¸è§çš„éœ€è¦è°ƒæ•´çš„å†…å®¹ï¼ŒåŒ…æ‹¬å„ç§çš„è·¯å¾„ã€è¶…å‚æ•°ã€è¯„ä¼°é—®é¢˜ç­‰ç­‰ã€‚

### 2.4 æ¨¡å‹è®­ç»ƒ

#### 2.4.1 å¸¸è§„è®­ç»ƒ

å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†é…ç½®æ–‡ä»¶å¥½ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` æŒ‡ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir` æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ï¼Œæ¯”å¦‚è¯´å°±ä¿å­˜åœ¨ `/root/ft/train` è·¯å¾„ä¸‹ã€‚å‡å¦‚ä¸æ·»åŠ çš„è¯æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹æ–‡ä»¶å°†é»˜è®¤ä¿å­˜åœ¨ `./work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy` çš„ä½ç½®ï¼Œå°±æ¯”å¦‚è¯´æˆ‘æ˜¯åœ¨ `/root/ft/train` çš„è·¯å¾„ä¸‹è¾“å…¥è¯¥æŒ‡ä»¤ï¼Œé‚£ä¹ˆæˆ‘çš„æ–‡ä»¶ä¿å­˜çš„ä½ç½®å°±æ˜¯åœ¨ `/root/ft/train/work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy` çš„ä½ç½®ä¸‹ã€‚

```bash
# æŒ‡å®šä¿å­˜è·¯å¾„
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train
```
åœ¨è¾“å…¥è®­ç»ƒå®Œåçš„æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š
```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
```

#### 2.4.2 ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»“åˆ XTuner å†…ç½®çš„ `deepspeed` æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„ `deepspeed` ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯ `deepspeed_zero1`, `deepspeed_zero2` å’Œ `deepspeed_zero3`

```bash
# ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train_deepspeed --deepspeed deepspeed_zero2
```
å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `deepspeed` æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº† `deepspeed` åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº†ä¸¤ä¸ª .pt æ–‡ä»¶ã€‚å½“ç„¶è¿™ä¸¤è€…åœ¨å…·ä½“çš„ä½¿ç”¨ä¸Šå¹¶æ²¡æœ‰å¤ªå¤§çš„å·®åˆ«ï¼Œéƒ½æ˜¯å¯ä»¥è¿›è¡Œè½¬åŒ–å¹¶æ•´åˆã€‚
![](images/image_04_16.png)


#### 2.4.3 è®­ç»ƒç»“æœ
ä½†æ˜¯å…¶å®æ— è®ºæ˜¯ç”¨å“ªç§æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°çš„ç»“æœéƒ½æ˜¯å¤§å·®ä¸å·®çš„ã€‚æˆ‘ä»¬ç”±äºè®¾ç½®äº†300è½®è¯„ä¼°ä¸€æ¬¡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¯¹æ¯”ä¸€ä¸‹300è½®å’Œ600è½®çš„è¯„ä¼°é—®é¢˜ç»“æœæ¥çœ‹çœ‹å·®åˆ«ã€‚
##### å¾®è°ƒå‰
![](images/image_04_13.png)
##### 300è½®
![](images/image_04_14.png)
> ä»è¿™ä¸ªç»“æœæ¥çœ‹ï¼Œ300è½®è¿­ä»£çš„æ—¶å€™å·²ç»***è¿‡æ‹Ÿåˆ***äº†ã€‚æˆ‘ä»¬ä¿®æ”¹configæ–‡ä»¶ï¼Œæ¯100è½®è¿­ä»£è¾“å‡ºä¸€æ¬¡
```bash
06/13 19:40:55 - mmengine - INFO - Iter(train) [100/768]  lr: 1.9491e-04  eta: 0:03:38  time: 0.3184  data_time: 0.0065  memory: 5661  loss: 0.0544
06/13 19:40:55 - mmengine - INFO - after_train_iter in EvaluateChatHook.
06/13 19:41:34 - mmengine - INFO - Sample output:
<s><|im_start|>user
è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°åŠ©æ‰‹å°èœé¸¡çš„å°

06/13 19:41:34 - mmengine - INFO - Sample output:
<s><|im_start|>user
ä½ æ˜¯è°<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹<|im_end|>

06/13 19:41:35 - mmengine - INFO - Sample output:
<s><|im_start|>user
ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—<|im_end|>
<|im_start|>assistant
æ˜¯çš„ï¼Œæˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹ã€‚<|im_end|>

06/13 19:41:35 - mmengine - INFO - Saving checkpoint at 100 iterations
[2024-06-13 19:41:40,075] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_100.pth is ready now!
06/13 19:41:43 - mmengine - INFO - Iter(train) [110/768]  lr: 1.9350e-04  eta: 0:08:04  time: 4.8321  data_time: 4.5121  memory: 5661  loss: 0.0584
06/13 19:41:46 - mmengine - INFO - Iter(train) [120/768]  lr: 1.9192e-04  eta: 0:07:34  time: 0.3171  data_time: 0.0063  memory: 5661  loss: 0.0415
06/13 19:41:49 - mmengine - INFO - Iter(train) [130/768]  lr: 1.9018e-04  eta: 0:07:08  time: 0.3054  data_time: 0.0062  memory: 5661  loss: 0.0484
06/13 19:41:52 - mmengine - INFO - Iter(train) [140/768]  lr: 1.8827e-04  eta: 0:06:44  time: 0.2953  data_time: 0.0059  memory: 5661  loss: 0.0302
06/13 19:41:55 - mmengine - INFO - Iter(train) [150/768]  lr: 1.8621e-04  eta: 0:06:23  time: 0.2890  data_time: 0.0059  memory: 5661  loss: 0.0285
06/13 19:41:58 - mmengine - INFO - Iter(train) [160/768]  lr: 1.8400e-04  eta: 0:06:04  time: 0.2840  data_time: 0.0064  memory: 5661  loss: 0.0227
06/13 19:42:01 - mmengine - INFO - Iter(train) [170/768]  lr: 1.8164e-04  eta: 0:05:47  time: 0.2881  data_time: 0.0066  memory: 5661  loss: 0.0232
06/13 19:42:03 - mmengine - INFO - Iter(train) [180/768]  lr: 1.7913e-04  eta: 0:05:32  time: 0.2830  data_time: 0.0058  memory: 5661  loss: 0.0170
06/13 19:42:06 - mmengine - INFO - Iter(train) [190/768]  lr: 1.7648e-04  eta: 0:05:18  time: 0.2889  data_time: 0.0095  memory: 5661  loss: 0.0164
06/13 19:42:09 - mmengine - INFO - Iter(train) [200/768]  lr: 1.7370e-04  eta: 0:05:04  time: 0.2826  data_time: 0.0059  memory: 5661  loss: 0.0109
06/13 19:42:09 - mmengine - INFO - after_train_iter in EvaluateChatHook.
06/13 19:42:10 - mmengine - INFO - Sample output:
<s><|im_start|>user
è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦<|im_end|>

06/13 19:42:12 - mmengine - INFO - Sample output:
<s><|im_start|>user
ä½ æ˜¯è°<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦<|im_end|>

06/13 19:42:13 - mmengine - INFO - Sample output:
<s><|im_start|>user
ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—<|im_end|>
<|im_start|>assistant
æˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦<|im_end|>
```
> å¯ä»¥çœ‹åˆ°ï¼Œå¤§çº¦åœ¨100-200è½®ä¹‹é—´è¾¾åˆ°æœ€ä¼˜


è¦è§£å†³è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹å¼è§£å†³ï¼š

1. **å‡å°‘ä¿å­˜æƒé‡æ–‡ä»¶çš„é—´éš”å¹¶å¢åŠ æƒé‡æ–‡ä»¶ä¿å­˜çš„ä¸Šé™**ï¼šè¿™ä¸ªæ–¹æ³•å®é™…ä¸Šå°±æ˜¯é€šè¿‡é™ä½é—´éš”ç»“åˆè¯„ä¼°é—®é¢˜çš„ç»“æœï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜çš„æƒé‡æ–‡ã€‚æˆ‘ä»¬å¯ä»¥æ¯éš”100ä¸ªæ‰¹æ¬¡æ¥çœ‹ä»€ä¹ˆæ—¶å€™æ¨¡å‹å·²ç»å­¦åˆ°äº†è¿™éƒ¨åˆ†çŸ¥è¯†ä½†æ˜¯è¿˜ä¿ç•™ç€åŸºæœ¬çš„å¸¸è¯†ï¼Œä»€ä¹ˆæ—¶å€™å·²ç»è¿‡æ‹Ÿåˆä¸¥é‡åªä¼šè¯´ä¸€å¥è¯äº†ã€‚ä½†æ˜¯ç”±äºå†é…ç½®æ–‡ä»¶æœ‰è®¾ç½®æƒé‡æ–‡ä»¶ä¿å­˜æ•°é‡çš„ä¸Šé™ï¼Œå› æ­¤åŒæ—¶å°†è¿™ä¸ªä¸Šé™åŠ å¤§ä¹Ÿæ˜¯éå¸¸å¿…è¦çš„ã€‚
2. **å¢åŠ å¸¸è§„çš„å¯¹è¯æ•°æ®é›†ä»è€Œç¨€é‡ŠåŸæœ¬æ•°æ®çš„å æ¯”**ï¼šè¿™ä¸ªæ–¹æ³•å…¶å®å°±æ˜¯å¸Œæœ›æˆ‘ä»¬æ­£å¸¸ç”¨å¯¹è¯æ•°æ®é›†åšæŒ‡ä»¤å¾®è°ƒçš„åŒæ—¶è¿˜åŠ ä¸Šä¸€éƒ¨åˆ†çš„æ•°æ®é›†æ¥è®©æ¨¡å‹æ—¢èƒ½å¤Ÿå­¦åˆ°æ­£å¸¸å¯¹è¯ï¼Œä½†æ˜¯åœ¨é‡åˆ°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œç‰¹æ®ŠåŒ–å¤„ç†ã€‚æ¯”å¦‚è¯´æˆ‘åœ¨ä¸€ä¸‡æ¡æ­£å¸¸çš„å¯¹è¯æ•°æ®é‡Œæ··å…¥ä¸¤åƒæ¡å’Œå°åŠ©æ‰‹ç›¸å…³çš„æ•°æ®é›†ï¼Œè¿™æ ·æ¨¡å‹åŒæ ·å¯ä»¥åœ¨ä¸ä¸¢å¤±å¯¹è¯èƒ½åŠ›çš„å‰æä¸‹å­¦åˆ°å°èœé¸¡çš„å°åŠ©æ‰‹è¿™å¥è¯ã€‚è¿™ç§å…¶å®æ˜¯æ¯”è¾ƒå¸¸è§çš„å¤„ç†æ–¹å¼ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±åŠ¨æ‰‹å°è¯•å®è·µä¸€ä¸‹ã€‚


### 2.5 æ¨¡å‹è½¬æ¢ã€æ•´åˆã€æµ‹è¯•åŠéƒ¨ç½²
#### 2.5.1 æ¨¡å‹è½¬æ¢
æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ Huggingface æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚

``` bash
# åˆ›å»ºä¸€ä¸ªä¿å­˜è½¬æ¢å Huggingface æ ¼å¼çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/huggingface

# æ¨¡å‹è½¬æ¢
# xtuner convert pth_to_hf ${é…ç½®æ–‡ä»¶åœ°å€} ${æƒé‡æ–‡ä»¶åœ°å€} ${è½¬æ¢åæ¨¡å‹ä¿å­˜åœ°å€}
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_768.pth /root/ft/huggingface
```
è¿™é‡Œå‘ç°æŠ¥é”™ï¼Œç¼ºå°‘æ–‡ä»¶ã€‚æŸ¥çœ‹å‘ç°æ˜¯å› ä¸ºä½¿ç”¨äº†deepspeedåŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼Œç»“æœæ–‡ä»¶é»˜è®¤å­˜åœ¨`trian_deepspeed`æ–‡ä»¶å¤¹ä¸‹è€Œä¸æ˜¯`train`,ä¿®æ”¹åé‡æ–°è¿è¡Œæ¨¡å‹è½¬æ¢å‘½ä»¤ã€‚  
![](images/image_04_15.png)
è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º Huggingface ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º Huggingface æ ¼å¼äº†ã€‚
```
|-- huggingface/
    |-- adapter_config.json
    |-- xtuner_config.py
    |-- adapter_model.bin
    |-- README.md
```

<span style="color: red;">**æ­¤æ—¶ï¼Œhuggingface æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**</span>

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

#### 2.5.2 æ¨¡å‹æ•´åˆ
æˆ‘ä»¬é€šè¿‡è§†é¢‘è¯¾ç¨‹çš„å­¦ä¹ å¯ä»¥äº†è§£åˆ°ï¼Œå¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆadapterï¼‰ã€‚é‚£ä¹ˆè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œç»„åˆæ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

è€Œå¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚

<img src="images/image_04_02.png" width="250" >


åœ¨ XTuner ä¸­ä¹Ÿæ˜¯æä¾›äº†ä¸€é”®æ•´åˆçš„æŒ‡ä»¤ï¼Œä½†æ˜¯åœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªåœ°å€ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„åœ°å€ã€è®­ç»ƒå¥½çš„ adapter å±‚çš„åœ°å€ï¼ˆè½¬ä¸º Huggingface æ ¼å¼åä¿å­˜çš„éƒ¨åˆ†ï¼‰ä»¥åŠæœ€ç»ˆä¿å­˜çš„åœ°å€ã€‚
```bash
# åˆ›å»ºä¸€ä¸ªåä¸º final_model çš„æ–‡ä»¶å¤¹å­˜å‚¨æ•´åˆåçš„æ¨¡å‹æ–‡ä»¶
mkdir -p /root/ft/final_model

# è§£å†³ä¸€ä¸‹çº¿ç¨‹å†²çªçš„ Bug 
export MKL_SERVICE_FORCE_INTEL=1

# è¿›è¡Œæ¨¡å‹æ•´åˆ
# xtuner convert merge  ${NAME_OR_PATH_TO_LLM} ${NAME_OR_PATH_TO_ADAPTER} ${SAVE_PATH} 
xtuner convert merge /root/ft/model /root/ft/huggingface /root/ft/final_model
```


æ•´åˆå®Œæˆåå¯ä»¥æŸ¥çœ‹åœ¨ final_model æ–‡ä»¶å¤¹ä¸‹çš„å†…å®¹ã€‚
![](images/image_04_16.png)

#### 2.5.3 å¯¹è¯æµ‹è¯•
åœ¨ XTuner ä¸­ä¹Ÿç›´æ¥çš„æä¾›äº†ä¸€å¥—åŸºäº transformers çš„å¯¹è¯ä»£ç ï¼Œè®©æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯ä¸ Huggingface æ ¼å¼çš„æ¨¡å‹è¿›è¡Œå¯¹è¯æ“ä½œã€‚æˆ‘ä»¬åªéœ€è¦å‡†å¤‡æˆ‘ä»¬åˆšåˆšè½¬æ¢å¥½çš„æ¨¡å‹è·¯å¾„å¹¶é€‰æ‹©å¯¹åº”çš„æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt-templateï¼‰å³å¯è¿›è¡Œå¯¹è¯ã€‚å‡å¦‚ prompt-template é€‰æ‹©æœ‰è¯¯ï¼Œå¾ˆæœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ­£ç¡®çš„è¿›è¡Œå›å¤ã€‚

> æƒ³è¦äº†è§£å…·ä½“æ¨¡å‹çš„ prompt-template æˆ–è€… XTuner é‡Œæ”¯æŒçš„ prompt-tempolateï¼Œå¯ä»¥åˆ° XTuner æºç ä¸­çš„ `xtuner/utils/templates.py` è¿™ä¸ªæ–‡ä»¶ä¸­è¿›è¡ŒæŸ¥æ‰¾ã€‚

```Bash
# ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/final_model --prompt-template internlm2_chat
```
æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›ç®€å•çš„æµ‹è¯•æ¥çœ‹çœ‹å¾®è°ƒåçš„æ¨¡å‹çš„èƒ½åŠ›ã€‚
> å‡å¦‚æˆ‘ä»¬æƒ³è¦è¾“å…¥å†…å®¹éœ€è¦åœ¨è¾“å…¥æ–‡å­—åæ•²å‡»ä¸¤ä¸‹å›è½¦ï¼Œå‡å¦‚æˆ‘ä»¬æƒ³æ¸…æ¥šå†å²è®°å½•éœ€è¦è¾“å…¥ RESETï¼Œå‡å¦‚æˆ‘ä»¬æƒ³è¦é€€å‡ºåˆ™éœ€è¦è¾“å…¥ EXITã€‚

![](images/image_04_17.png)
å¯ä»¥çœ‹åˆ°æ¨¡å‹å·²ç»ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œå›å¤çš„è¯å°±åªæœ‰ â€œæˆ‘æ˜¯å°èœé¸¡çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦â€ è¿™å¥è¯ã€‚æˆ‘ä»¬ä¸‹é¢å¯ä»¥é€šè¿‡å¯¹æ¯”åŸæ¨¡å‹çš„èƒ½åŠ›æ¥çœ‹çœ‹å·®å¼‚ã€‚

```bash
# åŒæ ·çš„æˆ‘ä»¬ä¹Ÿå¯ä»¥å’ŒåŸæ¨¡å‹è¿›è¡Œå¯¹è¯è¿›è¡Œå¯¹æ¯”
xtuner chat /root/ft/model --prompt-template internlm2_chat
```
æˆ‘ä»¬å¯ä»¥ç”¨åŒæ ·çš„é—®é¢˜æ¥æŸ¥çœ‹å›å¤çš„æƒ…å†µã€‚
![](images/image_04_18.png)  

å¯ä»¥çœ‹åˆ°åœ¨æ²¡æœ‰è¿›è¡Œæˆ‘ä»¬æ•°æ®çš„å¾®è°ƒå‰ï¼ŒåŸæ¨¡å‹æ˜¯èƒ½å¤Ÿè¾“å‡ºæœ‰é€»è¾‘çš„å›å¤ï¼Œå¹¶ä¸”ä¹Ÿä¸ä¼šè®¤ä¸ºä»–æ˜¯æˆ‘ä»¬ç‰¹æœ‰çš„å°åŠ©æ‰‹ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¾ˆæ˜æ˜¾çš„çœ‹å‡ºä¸¤è€…ä¹‹é—´çš„å·®å¼‚æ€§ã€‚

#### 2.5.4 Web demo éƒ¨ç½²

é™¤äº†åœ¨ç»ˆç«¯ä¸­å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨ç½‘é¡µç«¯çš„ demo è¿›è¡Œå¯¹è¯ã€‚

é‚£é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆä¸‹è½½ç½‘é¡µç«¯ web demo æ‰€éœ€è¦çš„ä¾èµ–ã€‚

```bash
pip install streamlit==1.24.0
```

ä¸‹è½½ [InternLM](https://github.com/InternLM/InternLM) é¡¹ç›®ä»£ç ï¼ˆæ¬¢è¿Starï¼‰ï¼


```shell
# åˆ›å»ºå­˜æ”¾ InternLM æ–‡ä»¶çš„ä»£ç 
mkdir -p /root/ft/web_demo && cd /root/ft/web_demo

# æ‹‰å– InternLM æºæ–‡ä»¶
git clone https://github.com/InternLM/InternLM.git

# è¿›å…¥è¯¥åº“ä¸­
cd /root/ft/web_demo/InternLM
```

å°† `/root/ft/web_demo/InternLM/chat/web_demo.py` ä¸­çš„å†…å®¹æ›¿æ¢ä¸ºä»¥ä¸‹çš„ä»£ç ï¼ˆä¸æºä»£ç ç›¸æ¯”ï¼Œæ­¤å¤„ä¿®æ”¹äº†æ¨¡å‹è·¯å¾„å’Œåˆ†è¯å™¨è·¯å¾„ï¼Œå¹¶ä¸”ä¹Ÿåˆ é™¤äº† avatar åŠ system_prompt éƒ¨åˆ†çš„å†…å®¹ï¼ŒåŒæ—¶ä¸ cli ä¸­çš„è¶…å‚æ•°è¿›è¡Œäº†å¯¹é½ï¼‰ã€‚


å°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨ PowerShell ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼ˆéœ€è¦æ›¿æ¢ä¸ºè‡ªå·±çš„ç«¯å£å·ï¼‰
```bash
# ä»æœ¬åœ°ä½¿ç”¨ ssh è¿æ¥ studio ç«¯å£
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 37028
```

ä¹‹åæˆ‘ä»¬éœ€è¦è¾“å…¥ä»¥ä¸‹å‘½ä»¤è¿è¡Œ `/root/personal_assistant/code/InternLM` ç›®å½•ä¸‹çš„ `web_demo.py` æ–‡ä»¶ã€‚

```bash
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```


æ‰“å¼€ [http://127.0.0.1:6006](http://127.0.0.1:6006) åï¼Œç­‰å¾…åŠ è½½å®Œæˆå³å¯è¿›è¡Œå¯¹è¯ï¼Œé”®å…¥å†…å®¹ç¤ºä¾‹å¦‚ä¸‹ï¼š

    è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±

æ•ˆæœå›¾å¦‚ä¸‹ï¼š

![image](images/image_04_19.png)

ä¿®æ”¹183è¡Œå’Œ186è¡Œçš„æ–‡ä»¶åœ°å€,å’ŒåŸæ¥çš„ InternLM2-Chat-1.8B æ¨¡å‹å¯¹è¯ã€‚

```diff
# ä¿®æ”¹æ¨¡å‹åœ°å€ï¼ˆç¬¬183è¡Œï¼‰
- model = (AutoModelForCausalLM.from_pretrained('/root/ft/final_model',
+ model = (AutoModelForCausalLM.from_pretrained('/root/ft/model',

# ä¿®æ”¹åˆ†è¯å™¨åœ°å€ï¼ˆç¬¬186è¡Œï¼‰
- tokenizer = AutoTokenizer.from_pretrained('/root/ft/final_model',
+ tokenizer = AutoTokenizer.from_pretrained('/root/ft/model',
```
ç„¶åä½¿ç”¨ä¸Šæ–¹åŒæ ·çš„å‘½ä»¤å³å¯è¿è¡Œã€‚

```bash
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

åŠ è½½å®Œæˆåè¾“å…¥åŒæ ·çš„é—®é¢˜ `è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±` ä¹‹åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ¨¡å‹æˆªç„¶ä¸åŒçš„å›å¤ï¼š

![image](images/image_04_20.png)
## è¿›é˜¶ä½œä¸š
### éƒ¨ç½²åˆ°OpenCLab
#### 1. åœ¨å¼€å‘æœºä¸Šé…ç½®git
```bash
# apt install git-lfs
apt-get install git-lfs

# use git install lfs
git lfs install

#è®¾ç½®OpenXLabçš„gitç”¨æˆ·å
git config --global user.name "User"
git config --global user.email "gmail.com"
```
#### 2. è¯­è¨€æ¨¡å‹ä»“åº“
åœ¨OpenXLabåˆ›å»ºç©ºä»“åº“
![](images/image_04_21.png)
å°†ä»“åº“å…‹éš†åˆ°å¼€å‘æœº
![](images/image_04_22.png)
```bash
cd ft/
mkdir openxlab && cd openxlab
git clone https://code.openxlab.org.cn/YZ-Li/XTuner_Demo.git
```

å°†final_modelä¸­çš„æ–‡ä»¶å¤åˆ¶åˆ°æœ¬åœ°ä»“åº“ä¸­
```bash
cp -r /root/ft/final_model/* /root/ft/openxlab/XTuner_Demo/
```
**LFSç®¡ç†å¤§æ–‡ä»¶ï¼š**ä½¿ç”¨ git lfs track å‘½ä»¤æ¥æ ‡è®°ä½ å¸Œæœ›é€šè¿‡ Git LFS ç®¡ç†çš„å¤§æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œæ‚¨æƒ³è¦é€šè¿‡LFSç®¡ç†æ‰€æœ‰çš„ .binå’Œ .modelçš„æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
```bash
git lfs track "*.bin"
git lfs track "*.model"
```
pushæœ¬åœ°ä»“åº“åˆ°è¿œç«¯
```bash
cd /root/ft/openxlab/XTuner_Demo/
git add -A
git commit -m "upload model"
git push
```
> æ‰§è¡Œ`git add`çš„æ—¶å€™ï¼Œä¼šå°†å·¥ä½œç›®å½•ä¸­æ‰€æœ‰å·²ä¿®æ”¹ã€å·²åˆ é™¤å’Œæ–°å¢çš„æ–‡ä»¶æ·»åŠ åˆ°æš‚å­˜åŒºã€‚å¯¹äºä½¿ç”¨ Git LFS ç®¡ç†çš„å¤§æ–‡ä»¶ï¼Œè¿™ä¸ªè¿‡ç¨‹è¿˜åŒ…æ‹¬å°†è¿™äº›å¤§æ–‡ä»¶ä¸Šä¼ åˆ° Git LFS æœåŠ¡å™¨ã€‚æ‰€ä»¥è¿™ä¸€æ­¥éœ€è¦å¾ˆä¹…ï¼Œè€å¿ƒç­‰å¾…...

> è¿™é‡Œéœ€è¦è¾“å…¥OpenXLabçš„ç”¨æˆ·åå’ŒAccess Token

#### 3. å‰ç«¯ä»“åº“

<span style="color: red;">**è¿™éƒ¨åˆ†å‰ç«¯å¿…é¡»åœ¨Githubé‡Œï¼Œä¸æ”¯æŒå…¶ä»–å¹³å°ï¼Œä¸æ˜¯åœ¨OpenXLabé‡Œ**</span>

ç„¶ååœ¨**Github**æ–°å»ºä¸€ä¸ªFrondendä»“åº“å¹¶å…‹éš†åˆ°æœ¬åœ°
```bash
cd  /root/ft/openxlab
git clone xxxx
cd xxxx
```
åœ¨æœ¬åœ°ä»“åº“æ–°å»º3ä¸ªæ–‡ä»¶ï¼Œ
```bash
touch app.py requirements.txt packages.txt
```
åœ¨`requirements.txt`æ–‡ä»¶ä¸­å†™å…¥pythonä¾èµ–
```bash
gradio==4.10.0
transformers
sentencepiece
einops
accelerate
tiktoken
```
åœ¨`packages.txt`æ–‡ä»¶ä¸­å†™å…¥Debianä¾èµ–
```bash
git
git-lfs
```
åœ¨`app.py`æ–‡ä»¶ä¸­å†™å…¥ä¸€ä¸‹å†…å®¹ã€‚**å¦‚æœQuotaä¸å¤Ÿä¸èƒ½ç”³è¯·åˆ°GPUï¼Œåˆ™éœ€è¦ä¿®æ”¹ä»£ç ï¼ŒåŠ è½½æ¨¡å‹åˆ°CPU**
```bash
import gradio as gr
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel

# download internlm2 to the base_path directory using git tool
base_path = './XTuner_Demo'
# è¿™é‡ŒGithubåœ°å€éœ€è¦æ”¹æˆè‡ªå·±çš„
os.system(f'git clone https://code.openxlab.org.cn/YZ-Li/XTuner_Demo.git {base_path}')
os.system(f'cd {base_path} && git lfs pull')

tokenizer = AutoTokenizer.from_pretrained(base_path,trust_remote_code=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForCausalLM.from_pretrained(base_path,trust_remote_code=True, torch_dtype=torch.float16).to(device)

def chat(message,history):
    for response,history in model.stream_chat(tokenizer,message,history,max_length=2048,top_p=0.7,temperature=1):
        yield response

gr.ChatInterface(chat,
                 title="InternLM2-Chat-7B",
                description="""
InternLM is mainly developed by Shanghai AI Laboratory.  
                 """,
                 ).queue(1).launch()
```

pushæ–‡æ¡£åˆ°è¿œç¨‹ä»“åº“
```bash
cd root//ft/openxlab/XTuner_Demo_Frontend
git add -A
git commit -m "add app.py requirements.txt packages.txt"
git push
```
æˆ‘çš„[Frontend Demo](https://github.com/LYZ-Li/XTuner_Demo_Frontend.git)çš„Githubä»“åº“ã€‚
#### 4. éƒ¨ç½²åº”ç”¨
https://openxlab.org.cn/apps/detail/YZ-Li/LYZ_XTuner_demo
![](images/image_04_23.png)
![](images/image_04_24.png)

### å¤ç°å¤šæ¨¡æ€å¾®è°ƒ
<span style="color: red;">**æœ¬éƒ¨åˆ†éœ€è¦çš„GPUèµ„æºä¸º24GB(30% çš„ A100)**</span>

#### 1. å®‰è£…ç¯å¢ƒ
æ²¿ç”¨ä¸Šé¢çš„ç¯å¢ƒå’Œ1.8Bæ¨¡å‹
```bash
conda activate xtuner0.1.17
```
#### 2. Pretrainé˜¶æ®µ
> è¿™ä¸€é˜¶æ®µä½¿ç”¨â€title +imageâ€œçš„æµ·é‡æ•°æ®åšè®­ç»ƒï¼Œå¯¹æ˜¾å­˜è¦æ±‚æå…¶é«˜ã€‚ç›®çš„æ˜¯è®©è¯­è¨€æ¨¡å‹â€çå¼€çœ¼ç›ï¼Œçœ‹æ‡‚å›¾ç‰‡â€œ

#### 2. Finetuneé˜¶æ®µ
> é¢„è®­ç»ƒåçš„`Image Projecter`åªèƒ½çœ‹åˆ°å›¾ç‰‡ï¼Œä½†æ˜¯ä¸èƒ½é’ˆå¯¹Userçš„é—®é¢˜åšå‡ºåˆç†å‡†ç¡®çš„å›ç­”ã€‚

##### 2.1 ç”Ÿæˆæ•°æ®

```bash
cd ~ && git clone https://github.com/InternLM/tutorial -b camp2 && conda activate xtuner0.1.17 && cd tutorial

python /root/tutorial/xtuner/llava/llava_data/repeat.py \
  -i /root/tutorial/xtuner/llava/llava_data/unique_data.json \
  -o /root/tutorial/xtuner/llava/llava_data/repeated_data.json \
  -n 200
```
æˆ‘æ˜¯æ‡’ç‹—ğŸ•æˆ‘ä½¿ç”¨å‡†å¤‡å¥½çš„configæ–‡ä»¶
```bash
cp /root/tutorial/xtuner/llava/llava_data/internlm2_chat_1_8b_llava_tutorial_fool_config.py /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py
```



<details>
<summary>é…ç½®æ–‡ä»¶ä¿®æ”¹è¯¦æƒ…</summary>

- pretrained_pth  
- llm_name_or_path  
- visual_encoder_name_or_path  
- data_root  
- data_path  
- image_folder  

```diff
# Model
- llm_name_or_path = 'internlm/internlm2-chat-1_8b'
+ llm_name_or_path = '/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b'
- visual_encoder_name_or_path = 'openai/clip-vit-large-patch14-336'
+ visual_encoder_name_or_path = '/root/share/new_models/openai/clip-vit-large-patch14-336'

# Specify the pretrained pth
- pretrained_pth = './work_dirs/llava_internlm2_chat_1_8b_clip_vit_large_p14_336_e1_gpu8_pretrain/iter_2181.pth'  # noqa: E501
+ pretrained_pth = '/root/share/new_models/xtuner/iter_2181.pth'

# Data
- data_root = './data/llava_data/'
+ data_root = '/root/tutorial/xtuner/llava/llava_data/'
- data_path = data_root + 'LLaVA-Instruct-150K/llava_v1_5_mix665k.json'
+ data_path = data_root + 'repeated_data.json'
- image_folder = data_root + 'llava_images'
+ image_folder = data_root

# Scheduler & Optimizer
- batch_size = 16  # per_device
+ batch_size = 1  # per_device


# evaluation_inputs
- evaluation_inputs = ['è¯·æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡','Please describe this picture']
+ evaluation_inputs = ['Please describe this picture','What is the equipment in the image?']

```
</details>

##### 2.2å¼€å§‹Finetune
```bash
# ä¿®æ”¹å¯¹åº”çš„ç‰ˆæœ¬
pip install transformers==4.36.0
cd /root/tutorial/xtuner/llava/
xtuner train /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py --deepspeed deepspeed_zero2
```

#### 3. å¯¹æ¯”Finetuneå‰å
##### 3.1 Finetuneå‰ï¼šåªä¼šæ‰“æ ‡é¢˜
```bash
# è§£å†³å°bug
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU

# pthè½¬huggingface
xtuner convert pth_to_hf \
  /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py \
  /root/share/new_models/xtuner/iter_2181.pth \
  /root/tutorial/xtuner/llava/llava_data/iter_2181_hf

# å¯åŠ¨ï¼
xtuner chat /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \
  --visual-encoder /root/share/new_models/openai/clip-vit-large-patch14-336 \
  --llava /root/tutorial/xtuner/llava/llava_data/iter_2181_hf \
  --prompt-template internlm2_chat \
  --image /root/tutorial/xtuner/llava/llava_data/test_img/oph.jpg
```
  
![](images/oph.jpg)
![](images/image_04_25.png)
  ##### 3.2 Finetuneåï¼šèƒ½å›ç­”é—®é¢˜
  ```bash
# è§£å†³å°bug
# è§£å†³å°bug
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU

# pthè½¬huggingface
xtuner convert pth_to_hf \
  /root/tutorial/xtuner/llava/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy.py \
  /root/tutorial/xtuner/llava/work_dirs/llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_copy/iter_1200.pth \
  /root/tutorial/xtuner/llava/llava_data/iter_1200_hf

# å¯åŠ¨ï¼
xtuner chat /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \
  --visual-encoder /root/share/new_models/openai/clip-vit-large-patch14-336 \
  --llava /root/tutorial/xtuner/llava/llava_data/iter_1200_hf \
  --prompt-template internlm2_chat \
  --image /root/tutorial/xtuner/llava/llava_data/test_img/oph.jpg
```
  ![](images/image_04_26.png)