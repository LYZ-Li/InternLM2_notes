# 

[书生·浦语大模型全链路开源体系](#书生·浦语大模型全链路开源体系)  
- [大模型成为发展人工智能的重要途经](##大模型成为发展人工智能的重要途经)
- [书生·浦语大模型开源历程](#书生浦语大模型开源历程)
- [InternLM2 的体系](#internlm2-的体系)
- [InternLM2的初心：回归语言建模本质](#internlm2的初心回归语言建模本质)
- [InternLM2主要亮点](#internlm2主要亮点)
- [从模型到应用典型流程](#从模型到应用典型流程)
- [全链条开源体系](#全链条开源体系)
  - [数据](#1-数据)
  - [预训练：InternLM2-Train](#2-预训练internlm2-train)
  - [微调：Xtuner](#3-微调xtuner)
  - [评测：OpenCompass司南大模型评测体系](#4-评测opencompass司南大模型评测体系)
  - [部署：LMDeploy](#5-部署lmdeploy)
  - [智能体框架：Lagent AGentLego](#5-智能体框架lagent-agentlego)

[论文阅读](#论文阅读):  
相关论文下载：
[InternLM2 技术报告](https://arxiv.org/pdf/2403.17297.pdf)

# 书生·浦语大模型全链路开源体系

## 大模型成为发展人工智能的重要途经
多任务多模态的通用大模型逐渐成为主流
![多任务多模态的通用大模型逐渐成为主流](/images/image_01_01.png)

## 书生·浦语大模型开源历程
![书生·浦语大模型开源历程](/images/image_01_02.png)

## InternLM2 的体系
![InternLM2 的体系](/images/image_01_03.png)

## InternLM2的初心：回归语言建模本质
也就是根据给出的context，预测接下里的token。训练这样一个模型的关键是获得高质量的语料库，为此开发使用了新一代数据清洗技术。  
![新一代数据清洗技术](/images/image_01_04.png)


## InternLM2主要亮点
![InternLM2主要亮点](/images/image_01_05.png)
模型的综合性能达到了同等重量级下的SOTA
![模型的综合性能达到了SOTA](/images/image_01_06.png)

## 从模型到应用典型流程
![典型流程](/images/image_01_07.png)


## 全链条开源体系
![全链条开源体系](/images/image_01_08.png)
### 1. 数据：
![数据](/images/image_01_09.png)
数据获取：[OpenDataLab](https://opendatalab.org.cn/)

### 2. 预训练：InternLM2-Train
![预训练](/images/image_01_10.png)
### 3. 微调：Xtuner
![微调](/images/image_01_11.png)
![微调：Xtuner](/images/image_01_12.png)
### 4. 评测：[OpenCompass司南大模型评测体系](https://rank.opencompass.org.cn/)
![评测：OpenCompass司南](/images/image_01_13.png)
洞见未来：国内模型的长处与短板
![洞见未来](/images/image_01_14.png)
### 5. 部署：LMDeploy
![部署：LMDeploy](/images/image_01_15.png)
### 5. 智能体框架：Lagent AGentLego
![Agent](/images/image_01_16.png)
实现框架与模块的解耦，兼容多种主流库
![解耦](/images/image_01_16.png)


# 论文阅读
#### 数据筛选和预训练

- **数据筛选**：使用两个分类器进行次级筛选，剔除低于阈值的数据，确保预训练数据的高质量。
- **编程数据**：对于LLM至关重要，支持编码辅助、软件开发等。通过训练代码数据可增强推理能力，因为代码比自然语言更结构化、严谨和可预测。
- **数据来源**：数据收集自GitHub爬取、公共数据集和编程相关的在线资源，如Q&A论坛、教程网站和API文档。
- **数据质量评估**：通过评分模型评估数据质量，高质量数据在预训练阶段具有更高的采样权重并进行多次训练，中等质量数据具有正常采样权重并训练一次。

#### 训练优化

- **训练速度**：由于InternEvo和flash注意力的良好可扩展性，上下文窗口从4K变化到32K时，训练速度仅下降40%。

#### 特定能力增强训练

- **关键能力**：推理、数学问题解决和知识记忆是LLM的关键能力，但相关高质量数据在整个语料库中稀疏分布。
- **数据集**：收集了一个包含240亿tokens的数据集，包含高质量检索数据和来自huggingface datasets平台的多种类型的开源数据。

#### 下游任务的性能评估

- **评估协议和性能指标**：对多个NLP任务的评估，包括综合考试、语言和知识、推理和数学、多种编程语言编码、长上下文建模和工具使用六个维度。
- **实验结果**：在多个选择题数据集（如MMLU）上进行基准测试，涵盖人文学科、社会科学、STEM等领域。使用LMDeploy推理引擎加速推理过程，展示InternLM2在长上下文建模方面的能力。

#### 工具使用

- **工具熟练度**：通过在GSM8K、Math、MathBench、T-Eval和CIBench的实验，分析了InternLM2在工具使用方面的熟练程度。

#### 条件奖励模型的消融研究

- **系统提示影响**：比较不同领域数据上的奖励模型在有无条件系统提示情况下的表现，结果表明没有系统提示会导致在多个公开数据集上的精度显著下降。

#### 总结

- **数据与训练**：精心策划的高质量数据集和特定的训练方法提升了模型在特定任务上的性能。
- **评估与实验**：通过详细评估和实验验证了InternLM2的有效性。